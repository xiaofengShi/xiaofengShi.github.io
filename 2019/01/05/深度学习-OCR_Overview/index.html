<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/panda_32px.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/panda_16px.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://xiaofengshi.com').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":"mac"},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta name="description" content="本篇涉及使用深度学习的方法实现字符识别的任务，该任务与计算机视觉领域内的图像描述任务(ImageCaption)相似，对于图像描述任务，输入为图像，输出为对该图像的描述；而对于OCR任务，输入的图像为包含文字或字符的图片，而输出为这张图片中的文字或字符。">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习-OCR_Overview">
<meta property="og:url" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/index.html">
<meta property="og:site_name" content="做一个有用的人">
<meta property="og:description" content="本篇涉及使用深度学习的方法实现字符识别的任务，该任务与计算机视觉领域内的图像描述任务(ImageCaption)相似，对于图像描述任务，输入为图像，输出为对该图像的描述；而对于OCR任务，输入的图像为包含文字或字符的图片，而输出为这张图片中的文字或字符。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/imagecaption.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ocr.jpg">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RRPN.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FTSN.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/DMPNet_anchors.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/EAST.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/TextBoxes.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/SegLink.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/TextBoxes++.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/PixelLink.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/WordSup.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/corner_overview.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/corner.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/overview.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ICDAR2013.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ICDAR2015.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/CRNN.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RARE_STN.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RARE_SRN.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FocusingAttention.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/AON.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ASTER_PARAMS.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FOTS.png">
<meta property="og:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/STN-OCR.png">
<meta property="article:published_time" content="2019-01-05T10:07:30.000Z">
<meta property="article:modified_time" content="2019-01-23T07:41:50.000Z">
<meta property="article:author" content="ShiXiaofeng">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="CV">
<meta property="article:tag" content="OCR">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/imagecaption.png">

<link rel="canonical" href="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>深度学习-OCR_Overview | 做一个有用的人</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">做一个有用的人</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <h1 class="site-subtitle" itemprop="description">LLM And Search</h1>
      
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-guestbook">

    <a href="/guestbook/" rel="section"><i class="fa fa-fw fa-commenting"></i>guestbook</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/xiaofengShi" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ShiXiaofeng">
      <meta itemprop="description" content="LLM,搜索,数据挖掘,深度学习相关技术记录分享">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="做一个有用的人">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          深度学习-OCR_Overview
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-01-05 18:07:30" itemprop="dateCreated datePublished" datetime="2019-01-05T18:07:30+08:00">2019-01-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-01-23 15:41:50" itemprop="dateModified" datetime="2019-01-23T15:41:50+08:00">2019-01-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/CV/" itemprop="url" rel="index">
                    <span itemprop="name">CV</span>
                  </a>
                </span>
            </span>

          
            <span id="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习-OCR_Overview" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本篇涉及使用深度学习的方法实现字符识别的任务，该任务与计算机视觉领域内的图像描述任务(<code>ImageCaption</code>)相似，对于图像描述任务，输入为图像，输出为对该图像的描述；而对于<code>OCR</code>任务，输入的图像为包含文字或字符的图片，而输出为这张图片中的文字或字符。</p>
<span id="more"></span>

<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/imagecaption.png" class="" title="ImageCaption">

<p>如图所示，对于<code>ImageCaption</code>任务来说，输入一张图片，输出对该图片的描述，简而言之就是看图说话。</p>
<hr>

<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ocr.jpg" class="" title="OCR">

<p>如图所示，对于<code>OCR</code>任务，输入的是一张身份张照片，输出的就是该照片中包含的文字内容。</p>
<hr>

<p>下面结合自己的开发经验对<code>OCR</code>项目的一些相关技术思路进行一些记录说明。</p>
<h2 id="技术方案"><a href="#技术方案" class="headerlink" title="技术方案"></a>技术方案</h2><h3 id="图像预处理"><a href="#图像预处理" class="headerlink" title="图像预处理"></a><strong>图像预处理</strong></h3><p>假如输入时一些扫描或相机拍摄的文档，首先第一点肯定是对图像进行一些预处理，比如灰度处理，二值化处理，仿射变换矫正等对图像进行预处理，使图像统一成设置好的规格。</p>
<h3 id="定位待识别图像"><a href="#定位待识别图像" class="headerlink" title="定位待识别图像"></a><strong>定位待识别图像</strong></h3><p>对图像进行预处理之后，接下来就是要找到图像中包含待识别文本的图像，一般分为两种思路，单字符提取和文本行提取</p>
<h4 id="单字符提取"><a href="#单字符提取" class="headerlink" title="单字符提取"></a><strong>单字符提取</strong></h4><p>这算是模式识别中比较常用的传统的方法，主要针对的印刷体文档图片，并且英文相比中文的提取效果很好(中文中包含很多左右文字结构，而不像英文都是使用空格进行各个字符分割)。具体的实现方法有如下几种：</p>
<ul>
<li>对图像进行二值化处理，仿射变换等预处理；对图像进行竖直方向投影，提取出文本行；对文本行在水平方向投影，提取出单个字符，以来设置的阈值进行字符提取，这里就可以看出对于中文来说，进行水平方向投影时，如果阈值设置的不合适，会存在将一个左右结构的汉字拆分成两个单独字符的风险，比如“从”可能会被提取成“人”“人”；</li>
<li>假定字符本身是具有连通性的，然后通过连通区域的检测方法找到文字字符的候选。</li>
<li>通过最大稳定极值区域（<code>MSER-Maximally Stable Extremal Regions</code>）得到字符的候选，并将这些字符候选看作连通图(graph)的顶点，此时就可以将文本行的寻找过程视为聚类（<code>clustering</code>）的过程，因为来自相同文本行的文本通常具有相同的方向、颜色、字体以及形状。最后使用一个文本分类器滤除非文本部分。</li>
<li>北科大殷绪成教授研究组的一个工作对文本的信息进行了更加全面的考虑，使用了文本的颜色、笔画宽度、字符方向（<code>orientation</code>）以及投影的特征。</li>
</ul>
<h4 id="文本行提取"><a href="#文本行提取" class="headerlink" title="文本行提取"></a><strong>文本行提取</strong></h4><p>由于外部因素和内部因素，场景文本检测具有一定的挑战性。外部因素源自环境，例如噪声、模糊和遮挡，它们也是一般目标检测中存在的主要问题。内部因素是由场景文本的属性和变化引起的。与一般目标检测相比，场景文本检测更加复杂，因为：</p>
<ol>
<li><p>场景文本可能以任意方向存在于自然图像中，因此边界框可能是旋转的矩形或者四边形；</p>
</li>
<li><p>场景文本边界框的长宽比变化比较大；</p>
</li>
<li><p>因为场景文本的形式可能是字符、单词或者文本行的形式，所以在定位边界的时候算法可能会发生混淆。</p>
</li>
</ol>
<p>基于一般目标检测和语义分割模型，几个精心设计的模型使得文本检测能够更加准确地进行。使用最广泛的是基于<code>RegionProposal</code>的方法，其次是基于图像分割的方法。文本检测模型的目标是从图片中尽可能准确地找出文字所在区域。</p>
<p>一般目标检测器（<code>SSD</code>，<code>YOLO</code> 和 <code>DenseBox</code> ）为基础，例如 <code>TextBoxes</code>，<code>FCRN</code> 以及 <code>EAST</code>，<code>SegLink</code>等，它们直接预测候选的边界框。</p>
<p>以语义分割为基础，例如<code>PixelLink</code>和<code>FTSN</code>，它们生成分割映射，然后通过后处理生成最终的文本边界框。</p>
<p>视觉领域常规物体检测方法(<code>SSD</code>, <code>YOLO</code>, <code>FasterRCNN</code>等)直接套用于文字检测任务效果并不理想， 主要原因如下：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">·相比于常规物体，文字行长度、长宽比例变化范围很大。</span><br><span class="line">·文本行是有方向性的。常规物体边框BBox的四元组描述方式信息量不充足。</span><br><span class="line">·自然场景中某些物体局部图像与字母形状相似，如果不参考图像全局信息将有误报。</span><br><span class="line">·有些艺术字体使用了弯曲的文本行，而手写字体变化模式也很多。</span><br><span class="line">·由于丰富的背景图像干扰，手工设计特征在自然场景文本识别任务中不够鲁棒。</span><br></pre></td></tr></table></figure>

<p>针对上述问题根因，近年来出现了各种基于深度学习的技术解决方案。它们从特征提取、区域建议网络(RPN)、多目标协同训练、Loss改进、非极大值抑制（NMS）、半监督学习等角度对常规物体检测方法进行改造，极大提升了自然场景图像中文本检测的准确率。例如：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">·CTPN方案中，用BLSTM模块提取字符所在图像上下文特征，以提高文本块识别精度。</span><br><span class="line">·RRPN等方案中，文本框标注采用BBOX +方向角度值的形式，模型中产生出可旋转的文字区域候选框，并	在边框回归计算过程中找到待测文本行的倾斜角度。</span><br><span class="line">·DMPNet等方案中，使用四边形（非矩形）标注文本框，来更紧凑的包围文本区域。</span><br><span class="line">·SegLink将单词切割为更易检测的小文字块，再预测邻近连接将小文字块连成词。</span><br><span class="line">·TextBoxes等方案中，调整了文字区域参考框的长宽比例，并将特征层卷积核调整为长方形，从而更适合检测出细长型的文本行。</span><br><span class="line">·FTSN方案中，作者使用Mask-NMS代替传统BBOX的NMS算法来过滤候选框。</span><br><span class="line">·WordSup方案中，采用半监督学习策略，用单词级标注数据来训练字符级文本检测模型。</span><br></pre></td></tr></table></figure>



<h5 id="CTPN-2016年"><a href="#CTPN-2016年" class="headerlink" title="CTPN(2016年)"></a><code>CTPN</code>(2016年)</h5><p><a href="https://arxiv.org/abs/1609.03605">论文链接Connectionist Text Proposal Network</a></p>
<p>该算法由华南理工大学金连文老师研究组提出，该算法脱胎于<code>FasterRcnn</code>，<code>CTPN</code>是目前流传最广、影响最大的开源文本检测模型，可以检测水平或微斜的文本行。在<code>FasterRcnn</code>基础上去掉了<code>ROI</code>层，引入了不同的<code>anchor</code>设置，<code>anchor</code>设置为固定宽度不同高度，来模拟不同高度的文本行，加入了<code>RNN</code>网络，使用<code>RNN</code>对目标的位置偏移和置信度得分的计算，具体的实现路径会在接下来进行详细的分析。该算法虽然很准，但是由于使用了<code>RNN</code>网络，拖慢了网络速度。</p>
<p>在<code>CTPN</code>中文本行可以被看成一个序列<code>sequence</code>，而不是一般物体检测中单个独立的目标。同一文本行上各个字符图像间可以互为上下文，在训练阶段让检测模型学习图像中蕴含的这种上下文统计规律，可以使得预测阶段有效提升文本块预测准确率。<code>CTPN</code>模型的图像预测流程中，basenet为<code>VGG16</code>做基础网络来提取各字符的局部图像特征，之后使用<code>BLSTM</code>层提取字符序列上下文特征，并根据<code>BLSTM</code>得到的feature为输入进行全连接输出，得到每个anchor的目标预测概率和预测的box，后续的处理部分和fasterRCNN网络的RPN网络相同，经过预测分支输出各个文字块的坐标值和分类结果概率值。在数据后处理阶段，将合并相邻的小文字块为文本行。</p>
<h5 id="RRPN-ECCV2017"><a href="#RRPN-ECCV2017" class="headerlink" title="RRPN(ECCV2017)"></a><code>RRPN</code>(ECCV2017)</h5><p><a href="https://arxiv.org/abs/1703.01086">Arbitrary-Oriented Scene Text Detection via Rotation Proposals</a></p>
<p><strong>这是第一个在场景文字检测中使用RNN的方法，但其主要用于水平文字的场景</strong>。基于旋转区域候选网络（<code>RRPN, Rotation Region Proposal Networks</code>）的方案，将旋转因素并入经典区域候选网络（如<code>FasterRCNN</code>）。这种方案中，一个文本区域的<code>ground truth</code>被表示为具有$5$元组$(x,y,h,w,\theta)$的旋转边框, 坐标$(x,y)$表示边框的几何中心, 高度$h$设定为边框的短边，宽度$w$为长边，方向是长边的方向。训练时，首先生成含有文本方向角的倾斜候选框，然后在边框回归过程中学习文本方向角。</p>
<p><code>RRPN</code>中方案中提出了旋转感兴趣区域（<code>RRoI，Rotation Region-of-Interest</code>）池化层，将任意方向的区域建议先划分成子区域，然后对这些子区域分别做<code>max pooling</code>、并将结果投影到具有固定空间尺寸小特征图上。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RRPN.png" class="" title="RRPN">

<hr>

<h5 id="FTSN-2018"><a href="#FTSN-2018" class="headerlink" title="FTSN(2018)"></a><code>FTSN</code>(2018)</h5><p><a href="https://arxiv.org/pdf/1709.03272.pdf">Fused Text Segmentation Networks for Multi-oriented Scene Text Detection</a></p>
<p><code>FTSN（Fused Text Segmentation Networks）</code>模型使用<strong>分割网络支持倾斜文本检测</strong>。它使用<code>Resnet-101</code>做基础网络，使用了多尺度融合的特征图。标注数据包括文本实例的像素掩码和边框，使用像素预测与边框检测多目标联合训练。</p>
<p>基于文本实例间像素级重合度的<code>Mask-NMS</code>， 替代了传统基于水平边框间重合度的<code>NMS</code>算法。下图左边子图是传统<code>NMS</code>算法执行结果，中间白色边框被错误地抑制掉了。下图右边子图是<code>Mask-NMS</code>算法执行结果， 三个边框都被成功保留下来。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FTSN.png" class="" title="FSTN">

<hr>

<h5 id="DMPNet-2017"><a href="#DMPNet-2017" class="headerlink" title="DMPNet(2017)"></a><code>DMPNet</code>(2017)</h5><p><a href="https://arxiv.org/pdf/1703.01425.pdf">Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection</a></p>
<p><code>DMPNet</code>（<code>Deep Matching Prior Network</code>）中，使用四边形（非矩形）来更紧凑地标注文本区域边界，其训练出的模型对倾斜文本块检测效果更好。</p>
<p>如下图所示，它使用滑动窗口在特征图上获取文本区域候选框，候选框既有正方形的、也有倾斜四边形的。接着，使用基于像素点采样的<code>Monte-Carlo</code>方法，来快速计算四边形候选框与标注框间的面积重合度。然后，计算四个顶点坐标到四边形中心点的距离，将它们与标注值相比计算出目标<code>Loss</code>。文章中推荐用<code>LnLoss</code>来取代<code>L1</code>、<code>L2Loss</code>，从而对大小文本框都有较快的训练回归速度。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/DMPNet_anchors.png" class="" title="DMPNet_anchors">

<hr>

<h5 id="EAST-2017"><a href="#EAST-2017" class="headerlink" title="EAST(2017)"></a><code>EAST</code>(2017)</h5><p><a href="https://arxiv.org/pdf/1704.03155.pdf">An Efficient and Accurate Scene Text Detector</a></p>
<p><code>EAST（An Efficient and Accurate Scene Text Detector）</code>模型中，首先使用全卷积网络（<code>FCN</code>）生成多尺度融合的特征图，作者使用了<code>PVANet</code>作为特征提取网络，然后在此基础上直接进行像素级的文本块预测。<strong>该模型中，支持旋转矩形框、任意四边形两种文本区域标注形式。</strong></p>
<ul>
<li>对应于四边形标注，模型执行时会对特征图中每个像素预测其到四个顶点的坐标差值。</li>
<li>对应于旋转矩形框标注，模型执行时会对特征图中每个像素预测其到矩形框四边的距离、以及矩形框的方向角。</li>
</ul>
<p>上述过程中，省略了其他模型中常见的区域建议、单词分割、子块合并等步骤，因此<strong>该模型的执行速度很快。</strong></p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/EAST.png" class="" title="EAST">

<hr>

<h5 id="Textboxes-2016"><a href="#Textboxes-2016" class="headerlink" title="Textboxes(2016)"></a><code>Textboxes(2016)</code></h5><p><a href="https://arxiv.org/pdf/1611.06779.pdf">TextBoxes: A Fast Text Detector with a Single Deep Neural Network</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/43545190">文本检测之TextBoxes</a></p>
<p><code>Textboxes</code>由华中科技大学的白翔组$2016$年提出。该模型是基于<code>SSD</code>框架的图文检测模型，训练方式是端到端的，运行速度也较快。在SSD基础上针对文字的形状做出了如下的改进，模型结构如下图所示</p>
<ul>
<li>为了适应文字行细长型的特点，候选框的长宽比增加了$1,2,3,5,7,10$这样初始值；</li>
<li>为了适应文本行细长型特点，特征层也用<strong>长条形卷积核</strong>代替了其他模型中常见的正方形($1 \times 1, 3\times 3$)卷积核；</li>
<li>为了防止漏检文本行，还在垂直方向增加了候选框数量；</li>
<li>为了检测大小不同的字符块，在多个尺度的特征图上并行预测文本框， 然后对预测结果做<code>NMS</code>过滤；</li>
<li>使用识别模型对文字进行过滤和判断，提出了一个实用的 “检测+识别”的框架。</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/TextBoxes.png" class="" title="TextBoxes">

<hr>

<h5 id="SegLink-2017"><a href="#SegLink-2017" class="headerlink" title="SegLink(2017)"></a><code>SegLink</code>(2017)</h5><p><a href="https://arxiv.org/pdf/1703.06520.pdf">论文链接 Detecting Oriented Text in Natural Images by Linking Segments</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37781277">文本检测之SegLink</a></p>
<p><a href="https://www.cnblogs.com/lillylin/p/6596731.html">白翔论文分析</a></p>
<p><code>SegLink</code>模型同样由白翔组提出，由<code>SSD</code>改进得来，<code>SegLink</code>模型的标注数据中，先将每个单词切割为更易检测的有方向的小文字块（$segment&#x3D;(x, y, w, h, \theta)$），然后用邻近连接（<code>link</code> ）将各个小文字块连接成单词。这种方案方便于识别长度变化范围很大的、带方向的单词和文本行，它不会像<code>FasterRCNN</code>等方案因为候选框长宽比例原因检测不出长文本行。<strong>相比于<code>CTPN</code>等文本检测模型，<code>SegLink</code>的图片处理速度快很多。</strong></p>
<ol>
<li>首先给定一个含有文本边界框（<code>bounding box</code>）的图片，先使用<code>CNN</code>提取图像的特征；</li>
<li>然后用<code>BLSTM</code>学习文字的空间上下文信息；</li>
<li>最后对特征进行编码并得到最终的预测结果。</li>
</ol>
<blockquote>
<p>与<code>CTPN</code>方法相比，<code>SegLink</code>引入了带方向的<code>bbox</code>(即文中说的<code>segment</code>$(x, y, w, h, \theta)$)，<strong>它可以检测任意方向的文本行，而<code>CTPN</code>主要用于检测水平的文本行，当然如果将垂直<code>anchor</code>改成水平<code>anchor</code>，也可以检测垂直方向的文本行</strong>；</p>
<p>与<code>EAST</code>方法相比，<code>SegLink</code>利用不同的<code>feature map</code>分别进行预测，而<code>EAST</code>是将不同的<code>feature map</code>层先进行合并再预测；</p>
<p>不能检测很大的文本，这是因为<code>link</code>主要是用于连接相邻的<code>segments</code>，而不能用于检测相距较远的文本行；</p>
<p>不能检测形变或者曲线文本，这是因为<code>segments combining</code>算法在合并的时候采用的是直线拟合．这里可以通过修改合并算法，来检测变形或曲线文本</p>
</blockquote>
<p>整个过程可以端到端（<code>end-to-end</code>）完成。<strong>提出的定位和识别模型结合之后能得到目前端到端模型中最好的文字检测结果。</strong></p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/SegLink.png" class="" title="SegLink">

<hr>

<h5 id="TextBoxes-2018"><a href="#TextBoxes-2018" class="headerlink" title="TextBoxes++(2018)"></a><code>TextBoxes++</code>(2018)</h5><p><a href="https://arxiv.org/pdf/1801.02765.pdf">TextBoxes++: A Single-Shot Oriented Scene Text Detector</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/33723456">论文笔记：TextBoxes++</a></p>
<p><a href="https://github.com/MhLiao/TextBoxes_plusplus">Github: TextBoxes++</a></p>
<p><code>Textboxes++</code>是<code>Textboxes</code>的升级版本，目的是增加对倾斜文本的支持。为此，将标注数据改为了旋转矩形框和不规则四边形的格式；对候选框的长宽比例、特征图层卷积核的形状都作了相应调整。·</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/TextBoxes++.png" class="" title="TextBoxes++">

<hr>

<h5 id="PixelLink-2018"><a href="#PixelLink-2018" class="headerlink" title="PixelLink(2018)"></a><code>PixelLink</code>(2018)</h5><p><a href="https://arxiv.org/pdf/1801.01315.pdf">论文链接 Detecting Scene Text via Instance Segmentation</a></p>
<p>自然场景图像中一组文字块经常紧挨在一起，通过语义分割方法很难将它们识别开来，所以阿里开发了<code>PixelLink</code>模型尝试用<strong>实例分割</strong>方法解决这个问题。</p>
<p>该模型的特征提取部分，为<code>VGG16</code>基础上构建的<code>FCN</code>网络。模型执行流程如下图所示。首先，借助于<code>CNN</code> 模块执行两个像素级预测：一个文本二分类预测，一个链接二分类预测。接着，用正链接去连接邻居正文本像素，得到文字块实例分割结果。然后，由分割结果直接就获得文字块边框， 而且允许生成倾斜边框。</p>
<p>上述过程中，<strong>省掉了其他模型中常见的边框回归步骤，因此训练收敛速度更快些</strong>。训练阶段，使用了平衡策略，使得每个文字块在总<code>LOSS</code>中的权值相同。训练过程中，<strong>通过预处理增加了各种方向角度的文字块实例。</strong></p>
<blockquote>
<ul>
<li><p>与CTPN，EAST，SegLink相比，PixelLink放弃了边框回归方法来检测文本行的bbox，而是采用实例分割方法，直接从分割的文本行区域得到文本行的bbox．PixelLink可以以更少额数据和更快地速度进行训练。</p>
</li>
<li><p>假设提取特征的主干网络结构采用VGG16(当然也可以采用其它主干网络结构)，PixelLink不需要在imagenet预训练的模型上进行fine-tuned（即直接从头开始训练），而CTPN，EAST，SegLink都需要在imagenet预训练的模型上进行fine-tuned；、</p>
</li>
<li><p>与CTPN，EAST，SegLink相比，PixelLink对感受野的要求更少，因为每个神经元值只负责预测自己及其邻域内的状态</p>
</li>
<li><p>与SegLink一样，不能检测很大的文本，这是因为link主要是用于连接相邻的segments，而不能用于检测相距较远的文本行</p>
</li>
</ul>
</blockquote>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/PixelLink.png" class="" title="PixelLink">

<hr>

<h5 id="WordSup-2017"><a href="#WordSup-2017" class="headerlink" title="WordSup(2017)"></a><code>WordSup</code>(2017)</h5><p><a href="https://arxiv.org/abs/1708.06720">WordSup: Exploiting Word Annotations for Character based Text Detection</a></p>
<p>百度提出，在数学公式图文识别、不规则形变文本行识别等应用中，字符级检测模型是一个关键基础模块。由于字符级自然场景图文标注成本很高、相关公开数据集稀少，导致现在多数图文检测模型只能在文本行、单词级标注数据上做训练。<code>WordSup</code>提出了一种弱监督的训练框架， 可以文本行、单词级标注数据集上训练出字符级检测模型。</p>
<p><code>WordSup</code>弱监督训练框架中，两个训练步骤被交替执行：给定当前字符检测模型，并结合单词级标注数据，计算出字符中心点掩码图； 给定字符中心点掩码图，有监督地训练字符级检测模型。</p>
<p>训练好字符检测器后，可以在数据流水线中加入合适的文本结构分析模块，以输出符合应用场景格式要求的文本内容。该文作者例举了多种文本结构分析模块的实现方法。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/WordSup.png" class="" title="WordSup">

<hr>

<h5 id="基于角定位与区域分割-2018"><a href="#基于角定位与区域分割-2018" class="headerlink" title="基于角定位与区域分割(2018)"></a>基于角定位与区域分割(2018)</h5><p><a href="https://arxiv.org/pdf/1802.08948.pdf">Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</a></p>
<p><strong>主要是为了解决场景文本多方向，长宽比变化较大等场景文本检测中的难点问题。</strong></p>
<p>该方法用一个端到端网络完成文字检测整个过程——除了基础卷积网络（<code>backbone</code>）外，包括两个并行分支和一个后处理。第一个分支是通过一个DSSD网络进行角点检测来提取候选文字区域，第二个分支是利用类似于<code>RFCN</code>进行网格划分的方式来做<code>position-sensitive</code>的<code>segmentation</code>。后处理是利用<code>segmentation</code>的<code>score map</code>的综合得分，过滤角点检测得到的候选区域中的噪声。</p>
<p><strong>文章亮点</strong></p>
<ul>
<li>检测不是用一般的<code>object detection</code>的框架来做，而是用<code>corner point detection</code>来做。（可以更好解决文字方向任意、文字长宽比很大的文本，不会受到感受野的影响）</li>
<li>分割用的是<code>position sensitive segmentation</code>，仿照<code>RFCN</code>划分网格的思路，把位置信息融合进去（对于检测单词这种细粒度的更有利）</li>
<li>把检测+分割两大类的方法整合起来，进行综合打分的<code>pipeline</code>（可以使得检测精度更高）</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/corner_overview.png" class="" title="Overview of our method. Given an image, the network outputs corner points and segmentation maps by corner detection and position-sensitive segmentation. Then candidate boxes are generated by sampling and grouping corner points. Finally, those candidate boxes are scored by segmentation maps and suppressed by NMS">

<ul>
<li><strong>backbone</strong>：<strong>基础网络，用来特征提取（不同分支特征共享）</strong></li>
<li><strong>corner detection：</strong>用来生成候选检测框，是一个独立的检测模块，类似于RPN的功能</li>
<li><strong>Position Sensitive Segmentation：</strong>整张图逐像素的打分，和一般分割不同的是输出4个score map，分别对应左上、左下、右上、右下不同位置的得分</li>
<li><strong>Scoring + NMS：</strong>综合打分，利用（2）的框和（3）的score map再综合打分，去掉非文字框，最后再接一个NMS</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/corner.png" class="" title="Network Architecture">

<ul>
<li><code>Backbone</code>取自DSSD &#x3D; VGG16(pool5) + conv6(fc6) + conv7(fc7) + 4conv + 6 deconv (with 6 residual block)</li>
<li><code>Corner Point Detection</code>是类似于<code>SSD</code>，从多个deconv的feature map上单独做detection得到候选框，然后多层的检测结果串起来nms后为最后的结果</li>
</ul>
<hr>

<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ul>
<li>多个数据集中不同模型的表现</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/overview.png" class="" title="overview">

<ul>
<li>ICDAR2013数据集表现</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ICDAR2013.png" class="" title="ICDAR2013">

<ul>
<li>ICDAR2015数据集表现</li>
</ul>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ICDAR2015.png" class="" title="ICDAR2015">

<p>图中蓝色为召回率(实际为真的样本中预测为真的概率，衡量检索的覆盖率)，绿色为精确率(预测为真的样本中实际为真的概率，衡量检测的信噪比)，橙色为Fscore，图中显示$0.00$的数据表示论文中没有给出相应得分。具体可以参考另一篇文章<a href="http://xiaofengshi.com/2018/11/20/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E7%82%B9/">机器学习-常用知识点</a>[可以直观的看到各个模型的评价指标值，在IC2015数据集上，<code>PixelLink_vgg2</code>对应的模型效果是最好的</p>
<h3 id="文本识别"><a href="#文本识别" class="headerlink" title="文本识别"></a><strong>文本识别</strong></h3><p>提取待识别图像之后就可以对图像进行识别了，和定位待识别图像相对应，同样是两种类型识别任务</p>
<h4 id="单字符识别"><a href="#单字符识别" class="headerlink" title="单字符识别"></a><strong>单字符识别</strong></h4><h5 id="k近邻"><a href="#k近邻" class="headerlink" title="k近邻"></a><strong>k近邻</strong></h5><p>该方法很好理解，数据库中保存着已经标注好label的不同的字符的图像，对于新的图像，识别时，比较待识别的图像和数据库中图像的距离，选择距离最小的类别对应的label为当前图像的识别结果，但是该方法存在一个问题，那就是计算量太大了，要想提高识别的准确性，就需要数据库中存储的数据要足够多，与此同时，在识别时计算量也会增加，识别一个图像要跟整个数据库中的所有样本进行距离计算</p>
<h5 id="图像分类模型"><a href="#图像分类模型" class="headerlink" title="图像分类模型"></a><strong>图像分类模型</strong></h5><p>借着深度学习的东风，使用图像识别算法进行单字符识别效果简直不要太好，相关的技术方法有很多，包括<code>VggNet</code>，<code>ResNet</code>，<code>InceptionNet</code>，<code>DenseNet</code>等，具体的可以参考本人的另一篇文字<a href="http://xiaofengshi.com/2018/11/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/">深度学习-图像识别</a>。</p>
<h4 id="不定长字符识别"><a href="#不定长字符识别" class="headerlink" title="不定长字符识别"></a><strong>不定长字符识别</strong></h4><p>目前基本都是使用的<code>CTC</code>策略(稍后详解)，对于特征提取的基础网络目前主要使用的是<code>CRNN</code>和<code>Densenet</code></p>
<h5 id="CRNN-CTC-2015"><a href="#CRNN-CTC-2015" class="headerlink" title="CRNN+CTC(2015)"></a><code>CRNN+CTC</code>(2015)</h5><p><a href="https://arxiv.org/pdf/1507.05717.pdf">An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition</a></p>
<p><strong><code>CRNN</code>(<code>Convolutional Recurrent Neural Network</code>）是目前较为流行的图文识别模型，可识别较长的文本序列。</strong></p>
<ul>
<li><p>包含<code>CNN</code>特征提取层和<code>BLSTM</code>序列特征提取层，能够进行端到端的联合训练。 </p>
</li>
<li><p>利用<code>BLSTM</code>和<code>CTC</code>部件学习字符图像中的上下文关系， 从而有效提升文本识别准确率，使得模型更加鲁棒。</p>
</li>
</ul>
<p>预测过程中，前端使用标准的<code>CNN</code>网络提取文本图像的特征，利用<code>BLSTM</code>将特征向量进行融合以提取字符序列的上下文特征，然后得到每列特征的概率分布，最后通过转录层(<code>CTC rule</code>)进行预测得到文本序列。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/CRNN.png" class="" title="CRNN">

<h5 id="DenseNet-CTC"><a href="#DenseNet-CTC" class="headerlink" title="DenseNet+CTC"></a><code>DenseNet+CTC</code></h5><p>该框架和<code>CRNN</code>相似，使用<code>DenseNet</code>替换了原来的<code>CNN+BLSM</code>，提升了模型的运行速度，但是DenseNet由于block层对特征图的重复利用会非常占显存。</p>
<h5 id="RARE-2016"><a href="#RARE-2016" class="headerlink" title="RARE(2016)"></a><code>RARE</code>(2016)</h5><p><a href="https://arxiv.org/pdf/1603.03915.pdf">Robust Scene Text Recognition with Automatic Rectification</a></p>
<p><strong><code>RARE</code>模型在识别变形的图像文本时效果很好。</strong>模型预测过程中，输入图像首先要被送到一个空间变换网络(<code>Spatial Transformer Network</code>)中做处理，矫正过的图像然后被送入序列识别网络中得到文本预测结果。空间变换网络如下图所示。网络模型有<code>STN+SRN</code>组成。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RARE_STN.png" class="" title="STNet">

<p><strong>空间变换网络内部包含定位网络(Localization Network)、网格生成器(Grid Generator)、采样器(Sampler)三个部件</strong>。经过训练后，它可以根据输入图像的特征图动态地产生空间变换网格，然后采样器根据变换网格核函数从原始图像中采样获得一个矩形的文本图像。<code>RARE</code>中支持一种称为<code>TPS（thin-plate splines）</code>的空间变换，从而能够比较准确地识别透视变换过的文本、以及弯曲的文本。</p>
<p>经过矫正之后的图像送入序列识别网络(<code>Sequence Recognition Network</code>)，网络结构如下图所示，为一个<code>Encoder_Decoder</code>模型，结构为<code>CNN+BLSTM+ATT_GRU+Softmax</code>，使用极小化对数似然估计的方法，优化方法为<code>ADADELTA</code></p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/RARE_SRN.png" class="" title="SRN">

<hr>

<h5 id="FocusingAttention-2017"><a href="#FocusingAttention-2017" class="headerlink" title=" FocusingAttention(2017)"></a><code> FocusingAttention</code>(2017)</h5><p><a href="https://arxiv.org/pdf/1709.02054.pdf">Focusing Attention: Towards Accurate Text Recognition in Natural Images</a></p>
<p><strong>利用<code>attention model</code>去做序列文字识别，可能会因为图像分辨率较低、遮挡、文字间间隔较大等问题而导致<code>attention</code>位置并不是很准，从而造成字符的错误识别</strong>。<strong>海康威视</strong>在ICCV2017上提出使用字符像素级别的监督信息使<code>attention</code>更加准确地聚焦在文字区域，从而使识别变得更精准。他们用了部分像素级别的标注，有了类别信息以后做多任务，结果较为精准。并且只要部分字符的标注就可以带来网络性能的一定提升。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FocusingAttention.png" class="" title="FocusingAttention">

<hr>

<h5 id="AON-2018"><a href="#AON-2018" class="headerlink" title="AON (2018)"></a>AON (2018)</h5><p><a href="https://arxiv.org/pdf/1711.04226.pdf">AON: Towards Arbitrarily-Oriented Text Recognition</a></p>
<p><strong>针对有形变或者任意方向文字的识别问题</strong>，Cheng等人在CVPR2018上提出了该模型。他们在水平方向之外加了一个竖直方向的双向LSTM，这样的话就有从上到下，从下到上，从左到右，从右到左四个方向序列的特征建模。接下来引入一个权重，该权重用来表示来自不同方向的特征在识别任务中发挥作用的重要性。这对性能有一定提升，尤其是对任意排列的文字识别。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/AON.png" class="" title="AON">

<h5 id="ASTER-2018"><a href="#ASTER-2018" class="headerlink" title="ASTER(2018)"></a>ASTER(2018)</h5><p><a href="https://www.researchgate.net/publication/325993414_ASTER_An_Attentional_Scene_Text_Recognizer_with_Flexible_Rectification">ASTER_An Attentional Scene Text Recognizer with Flexible Rectification</a></p>
<p>主要解决<strong>不规则排列文字</strong>的<strong>文字识别</strong>问题，论文为之前一篇<code>RARE</code>的改进版（journal版）</p>
<p><strong>主要思路</strong>：</p>
<ul>
<li>针对不规则文字，先矫正成正常线性排列的文字，再识别；</li>
<li>整合矫正网络和识别网络成为一个端到端网络来训练；</li>
<li>矫正网络使用STN，识别网络用经典的<code>seq2seq+attention</code></li>
</ul>
<p><strong>和<code>STN</code>的不同点</strong></p>
<p>本文在输入网络前将原图resize成小的图，然后在该小图上预测control point，而输入到Grid Generator或Sample计算的时候又映射回原图大小。这样的目的是<strong>为了减小网络参数，降低计算量</strong>（但有没有可能小图对于control point的prediction会不准？对于识别来讲，每个word的patch块本身就比较小了，而且小图映射回大图的点位置这个误差比例就会放大？）</p>
<p><strong>和<code>RARE</code>的不同点</strong></p>
<p>网络最后fc层的激活函数不是用tanh，而是直接对值进行clipping（具体怎么clip论文没说），这样做的目的是<strong>为了解决采样点可能落到图外面的问题，以及加快了网络训练的收敛速度</strong>，论文中对此没有解释本质原因，只是说明实验证明如此。</p>
<p>按照论文所述，ASTER和RARE之间的网络结构基本没有大幅度的变化，仍然是先矫正后识别，并且矫正模型和识别模型基本相同，但是效果相比RARE好了太多，不知为何。ASTER的网络参数配置如下所示</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/ASTER_PARAMS.png" class="" title="ASTER网络参数">

<hr>

<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p>目前主要的<code>OCR</code>项目使用的是如上的这些技术，比如<code>CTPN+CRNN</code>，<code>CTPN+DenseNet</code>，<code>YOLO+DenseNet</code>，<code>PixelLink+RARE</code>等，接下来针对上面这些技术点结合论文及代码进行一些个人的浅薄分析。</p>
<h3 id="End2End"><a href="#End2End" class="headerlink" title="End2End"></a><code>End2End</code></h3><p>端对端模型，直接从图片中定位并识别出包含的文本内容</p>
<h4 id="FOTS-2018"><a href="#FOTS-2018" class="headerlink" title="FOTS(2018)"></a><code>FOTS</code>(2018)</h4><p><a href="https://arxiv.org/pdf/1801.01671.pdf">FOTS: Fast Oriented Text Spotting with a Unified Network</a></p>
<p><code>FOTS</code>是图像文本检测与识别同步训练、端到端可学习的网络模型。检测和识别任务共享卷积特征层，既节省了计算时间，也比两阶段训练方式学习到更多图像特征。引入了旋转感兴趣区域（<code>RoIRotate</code>）, 可以从卷积特征图中产生出定向的文本区域，从而支持倾斜文本的识别。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/FOTS.png" class="" title="FOTS">

<hr>

<h4 id="STN-OCR-2017"><a href="#STN-OCR-2017" class="headerlink" title="STN-OCR(2017)"></a><code>STN-OCR</code>(2017)</h4><p><a href="https://arxiv.org/pdf/1707.08831.pdf">STN-OCR: A single Neural Network for Text Detection and Text Recognition</a></p>
<p><code>STN-OCR</code>是集成了了图文检测和识别功能的端到端可学习模型。在它的检测部分嵌入了一个空间变换网络（<code>STN</code>）来对原始输入图像进行仿射（<code>affine</code>）变换，类似于<code>RARE</code>中的<code>STN</code>网络。利用这个空间变换网络，可以对检测到的多个文本块分别执行旋转、缩放和倾斜等图形矫正动作，从而在后续文本识别阶段得到更好的识别精度。在训练上<code>STN-OCR</code>属于<strong>半监督学习方法，只需要提供文本内容标注，而不要求文本定位信息</strong>。作者也提到，<strong>如果从头开始训练则网络收敛速度较慢，因此建议渐进地增加训练难度</strong>。<code>STN-OCR</code>已经开放了工程源代码和预训练模型。</p>
<img src="/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/STN-OCR.png" class="" title="STN-OCR">

<hr>

<h4 id="Mask-TextSpotter-2018"><a href="#Mask-TextSpotter-2018" class="headerlink" title="Mask TextSpotter(2018)"></a>Mask TextSpotter(2018)</h4><p>该文受到<code>MaskRCNN</code>的启发提出了一种用于场景text spotting的可端到端训练的神经网络模型：M<code>ask TextSpotter</code>。与以前使用端到端可训练深度神经网络完成text spotting的方法不同，Mask TextSpotter利用简单且平滑的端到端学习过程，通过语义分割获得精确的文本检测和识别。此外，它在处理不规则形状的文本实例（例如，弯曲文本）方面优于之前的方法。</p>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><h2 id="训练数据集"><a href="#训练数据集" class="headerlink" title="训练数据集"></a><strong>训练数据集</strong></h2><p>本章将列举可用于文本检测和识别领域模型训练的一些大型公开数据集， 不涉及仅用于模型fine-tune任务的小型数据集。</p>
<h3 id="Chinese-Text-in-the-Wild-CTW"><a href="#Chinese-Text-in-the-Wild-CTW" class="headerlink" title="Chinese Text in the Wild(CTW)"></a><strong>Chinese Text in the Wild(CTW)</strong></h3><p>该数据集包含32285张图像，1018402个中文字符(来自于腾讯街景), 包含平面文本，凸起文本，城市文本，农村文本，低亮度文本，远处文本，部分遮挡文本。图像大小2048*2048，数据集大小为31GB。以(8:1:1)的比例将数据集分为训练集(25887张图像，812872个汉字)，测试集(3269张图像，103519个汉字)，验证集(3129张图像，103519个汉字)。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文献链接：https://arxiv.org/pdf/1803.00085.pdf </span><br><span class="line">数据集下载地址：https://ctwdataset.github.io/</span><br></pre></td></tr></table></figure>

<h3 id="Reading-Chinese-Text-in-the-Wild-RCTW-17"><a href="#Reading-Chinese-Text-in-the-Wild-RCTW-17" class="headerlink" title="Reading Chinese Text in the Wild(RCTW-17)"></a><strong>Reading Chinese Text in the Wild(RCTW-17)</strong></h3><p>该数据集包含12263张图像，训练集8034张，测试集4229张，共11.4GB。大部分图像由手机相机拍摄，含有少量的屏幕截图，图像中包含中文文本与少量英文文本。图像分辨率大小不等。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下载地址http://mclab.eic.hust.edu.cn/icdar2017chinese/dataset.html</span><br><span class="line">文献：http://arxiv.org/pdf/1708.09585v2</span><br></pre></td></tr></table></figure>

<h3 id="ICPR-MWI-2018-挑战赛"><a href="#ICPR-MWI-2018-挑战赛" class="headerlink" title="ICPR MWI 2018 挑战赛"></a>ICPR MWI 2018 挑战赛</h3><p>大赛提供20000张图像作为数据集，其中50%作为训练集，50%作为测试集。主要由合成图像，产品描述，网络广告构成。该数据集数据量充分，中英文混合，涵盖数十种字体，字体大小不一，多种版式，背景复杂。文件大小为2GB。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下载地址：</span><br><span class="line">https://tianchi.aliyun.com/competition/information.htm?raceId=231651&amp;_is_login_redirect=true&amp;accounttraceid=595a06c3-7530-4b8a-ad3d-40165e22dbfe   </span><br></pre></td></tr></table></figure>

<h3 id="Total-Text"><a href="#Total-Text" class="headerlink" title="Total-Text"></a><strong>Total-Text</strong></h3><p>该数据集共1555张图像，11459文本行，包含水平文本，倾斜文本，弯曲文本。文件大小441MB。大部分为英文文本，少量中文文本。训练集：1255张 测试集：300</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://www.cs-chan.com/source/ICDAR2017/totaltext.zip</span><br><span class="line">文献：http:// arxiv.org/pdf/1710.10400v</span><br></pre></td></tr></table></figure>

<h3 id="Google-FSNS-谷歌街景文本数据集"><a href="#Google-FSNS-谷歌街景文本数据集" class="headerlink" title="Google FSNS(谷歌街景文本数据集)"></a>Google FSNS(谷歌街景文本数据集)</h3><p>该数据集是从谷歌法国街景图片上获得的一百多万张街道名字标志，每一张包含同一街道标志牌的不同视角，图像大小为600*150，训练集1044868张，验证集16150张，测试集20404张。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://rrc.cvc.uab.es/?ch=6&amp;com=downloads</span><br><span class="line">文献：http:// arxiv.org/pdf/1702.03970v1</span><br></pre></td></tr></table></figure>

<h3 id="COCO-TEXT"><a href="#COCO-TEXT" class="headerlink" title="COCO-TEXT"></a><strong>COCO-TEXT</strong></h3><p>该数据集，包括63686幅图像，173589个文本实例，包括手写版和打印版，清晰版和非清晰版。文件大小12.58GB，训练集：43686张，测试集：10000张，验证集：10000张</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">文献: http://arxiv.org/pdf/1601.07140v2</span><br><span class="line">下载地址：https://vision.cornell.edu/se3/coco-text-2/</span><br></pre></td></tr></table></figure>

<h3 id="Synthetic-Data-for-Text-Localisation"><a href="#Synthetic-Data-for-Text-Localisation" class="headerlink" title="Synthetic Data for Text Localisation"></a><strong>Synthetic Data for Text Localisation</strong></h3><p>在复杂背景下人工合成的自然场景文本数据。包含858750张图像，共7266866个单词实例，28971487个字符，文件大小为41GB。该合成算法，不需要人工标注就可知道文字的label信息和位置信息，可得到大量自然场景文本标注数据。</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://www.robots.ox.ac.uk/~vgg/data/scenetext/</span><br><span class="line">文献：http://www.robots.ox.ac.uk/~ankush/textloc.pdf</span><br><span class="line">Code: https://github.com/ankush-me/SynthText (英文版)</span><br><span class="line">Code https://github.com/wang-tf/Chinese_OCR_synthetic_data(中文版)</span><br></pre></td></tr></table></figure>

<h3 id="Synthetic-Word-Dataset"><a href="#Synthetic-Word-Dataset" class="headerlink" title="Synthetic Word Dataset"></a><strong>Synthetic Word Dataset</strong></h3><p>合成文本识别数据集，包含9百万张图像，涵盖了9万个英语单词。文件大小为10GB</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://www.robots.ox.ac.uk/~vgg/data/text/</span><br></pre></td></tr></table></figure>

<h3 id="Caffe-ocr中文合成数据"><a href="#Caffe-ocr中文合成数据" class="headerlink" title="Caffe-ocr中文合成数据"></a><strong>Caffe-ocr中文合成数据</strong></h3><p>数据利用中文语料库，通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成，共360万张图片，图像分辨率为280x32，涵盖了汉字、标点、英文、数字共5990个字符。文件大小约为8.6GB</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">下载地址：https://pan.baidu.com/s/1dFda6R3</span><br></pre></td></tr></table></figure>

<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul>
<li><p>综述</p>
<p><a href="https://zhuanlan.zhihu.com/p/38655369">自然场景文本检测识别技术综述</a></p>
<p><a href="https://www.jiqizhixin.com/articles/2018-06-20-16">白翔:：图像OCR年度进展|VALSE2018之十一</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/29549641">白翔：趣谈“捕文捉字”– 场景文字检测 | VALSE2017之十</a></p>
<p><a href="https://blog.csdn.net/u013250416/article/details/79591263">基于深度学习的目标检测及场景文字检测研究进展</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/52335619">知乎文本检测综述</a></p>
<p><a href="https://www.cnblogs.com/lillylin/">优秀论文解读博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/burness-DL">知乎专栏:小石头的码疯窝</a></p>
</li>
<li><p>文本检测</p>
<ul>
<li><p>CTPN</p>
<p><a href="https://zhuanlan.zhihu.com/p/34757009">场景文字检测—CTPN原理与实现</a></p>
<p><a href="https://github.com/eragonruan/text-detection-ctpn">CTPN: Tensorflow</a></p>
</li>
<li><p>EAST</p>
<p><a href="https://www.cnblogs.com/lillylin/p/9954981.html">Bolg: EAST</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37504120">知乎：文本检测之EAST</a></p>
<p><a href="https://github.com/argman/EAST">EAST：tensorflow</a></p>
<p><a href="https://github.com/kurapan/EAST">EAST: Keras</a></p>
<p><a href="https://github.com/huoyijie/AdvancedEAST">EAST: Advanced keras</a></p>
</li>
<li><p>SegLink</p>
<p><a href="https://www.cnblogs.com/lillylin/p/6596731.html">SegLink_Blog</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37781277">文本检测之SegLink</a></p>
</li>
<li><p>PixelLink</p>
<p><a href="https://zhuanlan.zhihu.com/p/38171172">文本检测之PixelLink</a></p>
<p><a href="https://github.com/ZJULearning/pixel_link">Github: PixelLink </a></p>
</li>
<li><p>TextBoxes</p>
<p><a href="https://zhuanlan.zhihu.com/p/33723456">论文笔记：TextBoxes++: A Single-Shot Oriented Scene Text Detector</a></p>
<p><a href="https://github.com/MhLiao/TextBoxes_plusplus">Github: TextBoxes++</a></p>
</li>
<li><p>角定位</p>
</li>
</ul>
<p><a href="https://www.cnblogs.com/lillylin/p/8495124.html">基于角定位于区域分割</a></p>
</li>
<li><p>文本识别</p>
<ul>
<li><p>ASTER</p>
<p><a href="https://github.com/bgshih/aster">Github: ASTER</a></p>
</li>
</ul>
</li>
<li><p>TextSpotter</p>
<ul>
<li><p>Mask TextSpotter</p>
<p><a href="https://www.jiqizhixin.com/articles/2018-08-24-12">华科白翔教授团队ECCV2018 OCR论文：Mask TextSpotter</a></p>
</li>
</ul>
</li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div>赏杯咖啡！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechat.jpeg" alt="ShiXiaofeng 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpeg" alt="ShiXiaofeng 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>ShiXiaofeng
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://xiaofengshi.com/2019/01/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-OCR_Overview/" title="深度学习-OCR_Overview">http://xiaofengshi.com/2019/01/05/深度学习-OCR_Overview/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/CV/" rel="tag"># CV</a>
              <a href="/tags/OCR/" rel="tag"># OCR</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/01/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-TextDetection/" rel="prev" title="深度学习-TextDetection">
      <i class="fa fa-chevron-left"></i> 深度学习-TextDetection
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/12/12/Awesomes-Process/" rel="next" title="Awesomes-Process">
      Awesomes-Process <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%A1%88"><span class="nav-number">1.</span> <span class="nav-text">技术方案</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.1.</span> <span class="nav-text">图像预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9A%E4%BD%8D%E5%BE%85%E8%AF%86%E5%88%AB%E5%9B%BE%E5%83%8F"><span class="nav-number">1.2.</span> <span class="nav-text">定位待识别图像</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E5%AD%97%E7%AC%A6%E6%8F%90%E5%8F%96"><span class="nav-number">1.2.1.</span> <span class="nav-text">单字符提取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%A1%8C%E6%8F%90%E5%8F%96"><span class="nav-number">1.2.2.</span> <span class="nav-text">文本行提取</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CTPN-2016%E5%B9%B4"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">CTPN(2016年)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RRPN-ECCV2017"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">RRPN(ECCV2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FTSN-2018"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">FTSN(2018)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DMPNet-2017"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">DMPNet(2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#EAST-2017"><span class="nav-number">1.2.2.5.</span> <span class="nav-text">EAST(2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Textboxes-2016"><span class="nav-number">1.2.2.6.</span> <span class="nav-text">Textboxes(2016)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SegLink-2017"><span class="nav-number">1.2.2.7.</span> <span class="nav-text">SegLink(2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#TextBoxes-2018"><span class="nav-number">1.2.2.8.</span> <span class="nav-text">TextBoxes++(2018)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#PixelLink-2018"><span class="nav-number">1.2.2.9.</span> <span class="nav-text">PixelLink(2018)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#WordSup-2017"><span class="nav-number">1.2.2.10.</span> <span class="nav-text">WordSup(2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E8%A7%92%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2-2018"><span class="nav-number">1.2.2.11.</span> <span class="nav-text">基于角定位与区域分割(2018)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.2.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB"><span class="nav-number">1.3.</span> <span class="nav-text">文本识别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%95%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB"><span class="nav-number">1.3.1.</span> <span class="nav-text">单字符识别</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#k%E8%BF%91%E9%82%BB"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">k近邻</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">图像分类模型</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E5%AE%9A%E9%95%BF%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB"><span class="nav-number">1.3.2.</span> <span class="nav-text">不定长字符识别</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CRNN-CTC-2015"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">CRNN+CTC(2015)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DenseNet-CTC"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">DenseNet+CTC</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RARE-2016"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">RARE(2016)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#FocusingAttention-2017"><span class="nav-number">1.3.2.4.</span> <span class="nav-text"> FocusingAttention(2017)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#AON-2018"><span class="nav-number">1.3.2.5.</span> <span class="nav-text">AON (2018)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#ASTER-2018"><span class="nav-number">1.3.2.6.</span> <span class="nav-text">ASTER(2018)</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%BB%E7%BB%93-1"><span class="nav-number">1.3.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#End2End"><span class="nav-number">1.4.</span> <span class="nav-text">End2End</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#FOTS-2018"><span class="nav-number">1.4.1.</span> <span class="nav-text">FOTS(2018)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#STN-OCR-2017"><span class="nav-number">1.4.2.</span> <span class="nav-text">STN-OCR(2017)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mask-TextSpotter-2018"><span class="nav-number">1.4.3.</span> <span class="nav-text">Mask TextSpotter(2018)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.5.</span> <span class="nav-text">数据集</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.</span> <span class="nav-text">训练数据集</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Chinese-Text-in-the-Wild-CTW"><span class="nav-number">2.1.</span> <span class="nav-text">Chinese Text in the Wild(CTW)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reading-Chinese-Text-in-the-Wild-RCTW-17"><span class="nav-number">2.2.</span> <span class="nav-text">Reading Chinese Text in the Wild(RCTW-17)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ICPR-MWI-2018-%E6%8C%91%E6%88%98%E8%B5%9B"><span class="nav-number">2.3.</span> <span class="nav-text">ICPR MWI 2018 挑战赛</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Total-Text"><span class="nav-number">2.4.</span> <span class="nav-text">Total-Text</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Google-FSNS-%E8%B0%B7%E6%AD%8C%E8%A1%97%E6%99%AF%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">2.5.</span> <span class="nav-text">Google FSNS(谷歌街景文本数据集)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#COCO-TEXT"><span class="nav-number">2.6.</span> <span class="nav-text">COCO-TEXT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Synthetic-Data-for-Text-Localisation"><span class="nav-number">2.7.</span> <span class="nav-text">Synthetic Data for Text Localisation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Synthetic-Word-Dataset"><span class="nav-number">2.8.</span> <span class="nav-text">Synthetic Word Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Caffe-ocr%E4%B8%AD%E6%96%87%E5%90%88%E6%88%90%E6%95%B0%E6%8D%AE"><span class="nav-number">2.9.</span> <span class="nav-text">Caffe-ocr中文合成数据</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">3.</span> <span class="nav-text">Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="ShiXiaofeng"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">ShiXiaofeng</p>
  <div class="site-description" itemprop="description">LLM,搜索,数据挖掘,深度学习相关技术记录分享</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xiaofengShi" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xiaofengShi" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:sxf1052566766@163.com" title="E-Mail → mailto:sxf1052566766@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ShiXiaofeng</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v7.1.1
  </div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>




  
<script src="/js/local-search.js"></script>









<script>
if (document.querySelectorAll('div.pdf').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/pdfobject@2/pdfobject.min.js', () => {
    document.querySelectorAll('div.pdf').forEach(element => {
      PDFObject.embed(element.getAttribute('target'), element, {
        pdfOpenParams: {
          navpanes: 0,
          toolbar: 0,
          statusbar: 0,
          pagemode: 'thumbs',
          view: 'FitH'
        },
        PDFJS_URL: '/lib/pdf/web/viewer.html',
        height: element.getAttribute('height') || '500px'
      });
    });
  }, window.PDFObject);
}
</script>


<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme: 'forest',
      logLevel: 3,
      flowchart: { curve: 'linear' },
      gantt: { axisFormat: '%m/%d/%Y' },
      sequence: { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el: '#valine-comments',
      verify: false,
      notify: false,
      appId: 'rBlWbsM2JDda6CTwJlGaUFpI-gzGzoHsz',
      appKey: 'zJasiQnAqgUwf4kiHhHyXwOP',
      placeholder: "Leave Your Message and Contact Details",
      avatar: 'mm',
      meta: guest,
      pageSize: '10' || 10,
      visitor: true,
      lang: '' || 'zh-cn',
      path: location.pathname,
      recordIP: false,
      serverURLs: ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
